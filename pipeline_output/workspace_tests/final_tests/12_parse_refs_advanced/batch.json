[
  {
    "title": "",
    "doi": "10.48550/arXiv.1409.0473",
    "pdf_url": null,
    "arxiv_id": "1409.0473"
  },
  {
    "title": "",
    "doi": "10.48550/arXiv.2009.06732",
    "pdf_url": null,
    "arxiv_id": "2009.06732"
  },
  {
    "title": "",
    "doi": "10.5555/3295222.3295349",
    "pdf_url": null,
    "arxiv_id": null
  },
  {
    "title": "",
    "doi": "10.18653/v1/N19-1423",
    "pdf_url": null,
    "arxiv_id": null
  },
  {
    "title": "",
    "doi": "10.1145/3530811",
    "pdf_url": null,
    "arxiv_id": null
  },
  {
    "title": "",
    "doi": "10.1162/neco.1997.9.8.1735",
    "pdf_url": null,
    "arxiv_id": null
  },
  {
    "title": "Attention Is All You Need",
    "doi": null,
    "pdf_url": null,
    "arxiv_id": null
  },
  {
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "doi": null,
    "pdf_url": null,
    "arxiv_id": null
  },
  {
    "title": "Efficient Transformers: A Survey",
    "doi": null,
    "pdf_url": null,
    "arxiv_id": null
  },
  {
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "doi": null,
    "pdf_url": null,
    "arxiv_id": null
  },
  {
    "title": "Long Short-Term Memory",
    "doi": null,
    "pdf_url": null,
    "arxiv_id": null
  },
  {
    "title": "Language Models are Few-Shot Learners",
    "doi": null,
    "pdf_url": null,
    "arxiv_id": null
  },
  {
    "title": "Multi-Head Attention Mechanism",
    "doi": null,
    "pdf_url": null,
    "arxiv_id": null
  }
]