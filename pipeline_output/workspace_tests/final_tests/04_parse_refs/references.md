# Extracted References


## Arxiv (2)

- [1409.0473](https://arxiv.org/abs/1409.0473)
- [2009.06732](https://arxiv.org/abs/2009.06732)

## Doi (4)

- [10.5555/3295222.3295349](https://doi.org/10.5555/3295222.3295349)
- [10.18653/v1/N19-1423](https://doi.org/10.18653/v1/N19-1423)
- [10.1145/3530811](https://doi.org/10.1145/3530811)
- [10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)

## Paper (7)

- Attention Is All You Need
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- Efficient Transformers: A Survey
- Neural Machine Translation by Jointly Learning to Align and Translate
- Long Short-Term Memory
- Language Models are Few-Shot Learners
- Multi-Head Attention Mechanism

## Website (5)

- [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)
  - jalammar.github.io
- [https://kazemnejad.com/blog/transformer_architecture_positional_encoding/](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
  - kazemnejad.com
- [https://medium.com/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b](https://medium.com/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b)
  - medium.com
- [https://blog.stackademic.com/how-rnns-were-replaced-by-transformers-and-why-8b60ac729b80](https://blog.stackademic.com/how-rnns-were-replaced-by-transformers-and-why-8b60ac729b80)
  - blog.stackademic.com
- [https://www.geeksforgeeks.org/nlp/multi-head-attention-mechanism/](https://www.geeksforgeeks.org/nlp/multi-head-attention-mechanism/)
  - www.geeksforgeeks.org