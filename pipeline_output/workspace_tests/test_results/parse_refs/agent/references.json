[
  {
    "type": "paper",
    "value": "Attention Is All You Need",
    "title": "Attention Is All You Need",
    "authors": "Vaswani et al.",
    "year": "2017",
    "url": "https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html"
  },
  {
    "type": "website",
    "value": "https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html",
    "title": "Attention Is All You Need",
    "authors": "Vaswani et al.",
    "year": "2017",
    "url": "https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html"
  },
  {
    "type": "paper",
    "value": "BERT: Pre-training of Deep Bidirectional Transformers",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers",
    "authors": "Devlin et al.",
    "year": "2019",
    "url": "https://aclanthology.org/N19-1423"
  },
  {
    "type": "doi",
    "value": "10.18653/v1/N19-1423",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers",
    "authors": "Devlin et al.",
    "year": "2019",
    "url": "https://doi.org/10.18653/v1/N19-1423"
  },
  {
    "type": "website",
    "value": "https://aclanthology.org/N19-1423",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers",
    "authors": "Devlin et al.",
    "year": "2019",
    "url": "https://aclanthology.org/N19-1423"
  },
  {
    "type": "paper",
    "value": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "authors": "Dosovitskiy et al.",
    "year": "2021",
    "url": "https://openreview.net/forum?id=YicbFdNTTy"
  },
  {
    "type": "website",
    "value": "https://openreview.net/forum?id=YicbFdNTTy",
    "title": "An Image is Worth 16x16 Words",
    "authors": "Dosovitskiy et al.",
    "year": "2021",
    "url": "https://openreview.net/forum?id=YicbFdNTTy"
  },
  {
    "type": "paper",
    "value": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
    "authors": "Liu et al.",
    "year": "2021",
    "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf"
  },
  {
    "type": "doi",
    "value": "10.1109/ICCV48922.2021.00986",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
    "authors": "Liu et al.",
    "year": "2021",
    "url": "https://doi.org/10.1109/ICCV48922.2021.00986"
  },
  {
    "type": "pdf",
    "value": "https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
    "authors": "Liu et al.",
    "year": "2021",
    "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf"
  },
  {
    "type": "arxiv",
    "value": "2303.08774",
    "title": "GPT-4 Technical Report",
    "authors": "OpenAI",
    "year": "2023",
    "url": "https://arxiv.org/abs/2303.08774"
  },
  {
    "type": "arxiv",
    "value": "2407.21783",
    "title": "The Llama 3 Herd of Models",
    "authors": "Dubey et al.",
    "year": "2024",
    "url": "https://arxiv.org/abs/2407.21783"
  },
  {
    "type": "arxiv",
    "value": "2302.13971",
    "title": "LLaMA: Open and Efficient Foundation Language Models",
    "authors": "Touvron et al.",
    "year": "2023",
    "url": "https://arxiv.org/abs/2302.13971"
  },
  {
    "type": "website",
    "value": "https://jalammar.github.io/illustrated-transformer/",
    "title": "The Illustrated Transformer",
    "authors": "Jay Alammar",
    "year": "2018",
    "url": "https://jalammar.github.io/illustrated-transformer/"
  },
  {
    "type": "website",
    "value": "http://nlp.seas.harvard.edu/2018/04/03/attention.html",
    "title": "The Annotated Transformer",
    "authors": "Harvard NLP",
    "year": "2018",
    "url": "http://nlp.seas.harvard.edu/2018/04/03/attention.html"
  },
  {
    "type": "website",
    "value": "https://towardsdatascience.com/transformers-from-nlp-to-computer-vision-4f237386610c/",
    "title": "Transformers from NLP to Computer Vision",
    "authors": "",
    "year": "2024",
    "url": "https://towardsdatascience.com/transformers-from-nlp-to-computer-vision-4f237386610c/"
  },
  {
    "type": "github",
    "value": "pytorch/pytorch",
    "title": "PyTorch",
    "authors": "",
    "year": "",
    "url": "https://github.com/pytorch/pytorch"
  },
  {
    "type": "github",
    "value": "huggingface/transformers",
    "title": "Hugging Face Transformers",
    "authors": "",
    "year": "",
    "url": "https://github.com/huggingface/transformers"
  },
  {
    "type": "github",
    "value": "microsoft/Swin-Transformer",
    "title": "Swin-Transformer",
    "authors": "",
    "year": "",
    "url": "https://github.com/microsoft/Swin-Transformer"
  },
  {
    "type": "website",
    "value": "https://huggingface.co/docs/transformers",
    "title": "Transformers Documentation",
    "authors": "Hugging Face",
    "year": "",
    "url": "https://huggingface.co/docs/transformers"
  },
  {
    "type": "website",
    "value": "https://pytorch.org/tutorials/beginner/transformer_tutorial.html",
    "title": "PyTorch Transformer Tutorial",
    "authors": "",
    "year": "",
    "url": "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
  },
  {
    "type": "website",
    "value": "https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md",
    "title": "Llama 3 Model Card",
    "authors": "Meta AI",
    "year": "",
    "url": "https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md"
  },
  {
    "type": "youtube",
    "value": "kCc8FmEb1nY",
    "title": "Let's build GPT: from scratch, in code, spelled out",
    "authors": "Andrej Karpathy",
    "year": "2023",
    "url": "https://www.youtube.com/watch?v=kCc8FmEb1nY"
  },
  {
    "type": "youtube",
    "value": "iDulhoQ2pro",
    "title": "Attention is All You Need - Paper Explained",
    "authors": "Yannic Kilcher",
    "year": "2020",
    "url": "https://www.youtube.com/watch?v=iDulhoQ2pro"
  },
  {
    "type": "youtube",
    "value": "wjZofJX0v4M",
    "title": "Transformers, the tech behind LLMs",
    "authors": "3Blue1Brown",
    "year": "2024",
    "url": "https://www.youtube.com/watch?v=wjZofJX0v4M"
  },
  {
    "type": "website",
    "value": "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)",
    "title": "Transformer (machine learning model)",
    "authors": "",
    "year": "",
    "url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)"
  },
  {
    "type": "paper",
    "value": "Array programming with NumPy",
    "title": "Array programming with NumPy",
    "authors": "Harris et al.",
    "year": "2020",
    "url": "https://doi.org/10.1038/s41586-020-2649-2"
  },
  {
    "type": "doi",
    "value": "10.1038/s41586-020-2649-2",
    "title": "Array programming with NumPy",
    "authors": "Harris et al.",
    "year": "2020",
    "url": "https://doi.org/10.1038/s41586-020-2649-2"
  },
  {
    "type": "paper",
    "value": "Deep Learning",
    "title": "Deep Learning",
    "authors": "LeCun et al.",
    "year": "2015",
    "url": "https://doi.org/10.1038/nature14539"
  },
  {
    "type": "doi",
    "value": "10.1038/nature14539",
    "title": "Deep Learning",
    "authors": "LeCun et al.",
    "year": "2015",
    "url": "https://doi.org/10.1038/nature14539"
  }
]